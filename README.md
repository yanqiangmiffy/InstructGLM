# InstructGLM

> åŸºäºChatGLM-6B+LoRAåœ¨æŒ‡ä»¤æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒ
> 
> 
**æœ¬é¡¹ç›®ä¸»è¦å†…å®¹ï¼š**

- ğŸš€ é’ˆå¯¹ChatGLM-6Bæ¨¡å‹åŸºäºLoRAæŠ€æœ¯è¿›è¡Œå¾®è°ƒ
- ğŸš€ å¼€æºäº†åŸºäºalpacaå’Œbelleæ•°æ®æŒ‡ä»¤å¾®è°ƒåçš„loraæƒé‡ï¼Œè¯¦æƒ…å¯è§[output](https://github.com/yanqiangmiffy/InstructGLM/tree/master/output)
- ğŸš€ åŸºäºdeepspeedæ”¯æŒå¤šå¡å¾®è°ƒï¼Œå…·ä½“è®¾ç½®å¯è§ [å¾®è°ƒ3:åŸºäºDeepSpeedè¿›è¡ŒLoraå¾®è°ƒ](#å¾®è°ƒ3:åŸºäºDeepSpeedè¿›è¡ŒLoraå¾®è°ƒ)
- ğŸš€ åŸºäºgradioçš„demoå®Œå–„
## å¼€æºæŒ‡ä»¤æ•°æ®é›†

- [æ–¯å¦ç¦52kè‹±æ–‡æŒ‡ä»¤æ•°æ®](https://github.com/tatsu-lab/stanford_alpaca)

> instruction:52K æ¡æŒ‡ä»¤ä¸­çš„æ¯ä¸€æ¡éƒ½æ˜¯å”¯ä¸€çš„,ç­”æ¡ˆç”±text-davinci-003æ¨¡å‹ç”Ÿæˆå¾—åˆ°çš„

- [BELLEé¡¹ç›®ç”Ÿæˆçš„ä¸­æ–‡æŒ‡ä»¤æ•°æ®ï¼š0.5m&1m](https://huggingface.co/datasets/BelleGroup/generated_train_0.5M_CN)

> 1ç™¾ä¸‡æ•°æ®ï¼šhttps://huggingface.co/datasets/BelleGroup/generated_train_1M_CN

> ç”Ÿæˆæ–¹å¼åŸºäºç§å­promptï¼Œè°ƒç”¨openaiçš„apiç”Ÿæˆä¸­æ–‡æŒ‡ä»¤

- [GuanacoDataset å¤šè¯­è¨€æŒ‡ä»¤æ•°æ®é›†](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)

> Guanaco æ˜¯åœ¨ Meta çš„ LLaMA 7B æ¨¡å‹ä¸Šè®­ç»ƒçš„æŒ‡ä»¤è·Ÿéšè¯­è¨€æ¨¡å‹ã€‚åœ¨ Alpaca æ¨¡å‹åŸå§‹ 52K æ•°æ®çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æ·»åŠ äº†é¢å¤–çš„ 98,369 ä¸ªæ¡ç›®ï¼Œæ¶µç›–è‹±è¯­ã€ç®€ä½“ä¸­æ–‡ã€ç¹ä½“ä¸­æ–‡ï¼ˆå°æ¹¾ï¼‰ã€ç¹ä½“ä¸­æ–‡ï¼ˆé¦™æ¸¯ï¼‰ã€æ—¥è¯­ã€å¾·è¯­ä»¥åŠå„ç§è¯­è¨€å’Œè¯­æ³•ä»»åŠ¡ã€‚é€šè¿‡ä½¿ç”¨è¿™äº›ä¸°å¯Œçš„æ•°æ®é‡æ–°è®­ç»ƒå’Œä¼˜åŒ–æ¨¡å‹ï¼ŒGuanaco åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­å±•ç¤ºäº†å‡ºè‰²çš„æ€§èƒ½å’Œæ½œåŠ›ã€‚é¡¹ç›®é“¾æ¥å¯ä»¥æŸ¥çœ‹
> https://guanaco-model.github.io/

- [alpacaä¸­æ–‡æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†](https://github.com/carbonz0/alpaca-chinese-dataset)

> ä¸åŸå§‹alpacaæ•°æ®jsonæ ¼å¼ç›¸åŒ,æ•°æ®ç”Ÿæˆçš„æ–¹æ³•æ˜¯æœºå™¨ç¿»è¯‘å’Œself-instruct

- [äººå·¥ç²¾è°ƒçš„ä¸­æ–‡å¯¹è¯æ•°æ®é›†](https://github.com/hikariming/alpaca_chinese_dataset)
>åŠ å…¥é™¤äº†alpacaä¹‹å¤–çš„å…¶ä»–ä¸­æ–‡èŠå¤©å¯¹è¯
äººå·¥å¾®è°ƒï¼Œéƒ¨åˆ†å¹¶ä¸ä¸­æ–‡åŒ–çš„é—®é¢˜ï¼Œæˆ‘ä»¬å°†é‡æ–°è¯¢é—®chatgptæˆ–æ–‡å¿ƒä¸€è¨€ï¼Œé‡æ–°è·å–å›ç­”å¹¶è¦†ç›–æ‰alpacaçš„å›ç­”


- [firefly-train-1.1M](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M) ï¼Œ ä¸€ä»½é«˜è´¨é‡çš„åŒ…å«1.1Mä¸­æ–‡å¤šä»»åŠ¡æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼ŒåŒ…å«23ç§å¸¸è§çš„ä¸­æ–‡NLPä»»åŠ¡çš„æŒ‡ä»¤æ•°æ®ã€‚å¯¹äºæ¯ä¸ªä»»åŠ¡ï¼Œç”±äººå·¥ä¹¦å†™è‹¥å¹²æŒ‡ä»¤æ¨¡æ¿ï¼Œä¿è¯æ•°æ®çš„é«˜è´¨é‡ä¸ä¸°å¯Œåº¦ã€‚

## å¾®è°ƒ1ï¼šalpacaè‹±æ–‡æŒ‡ä»¤æ•°æ®

æ–¯å¦ç¦ç¾Šé©¼52kæ•°æ®ï¼ŒåŸå§‹æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼š

```text
{
    "instruction": "Evaluate this sentence for spelling and grammar mistakes",
    "input": "He finnished his meal and left the resturant",
    "output": "He finished his meal and left the restaurant."
}
```

> æ•°æ®é›†åœ°å€ï¼šhttps://github.com/tatsu-lab/stanford_alpaca

### 1.æ•°æ®é¢„å¤„ç†

è½¬åŒ–alpacaæ•°æ®é›†ä¸ºjsonl,è¿™ä¸€æ­¥å¯ä»¥æ‰§è¡Œè®¾ç½®æ•°æ®è½¬æ¢åæ ¼å¼ï¼Œæ¯”å¦‚ï¼š

```text
###Instruction:xxx###Input:xxxx###Response:xxx
```

```shell
python cover_alpaca2jsonl.py \
    --data_path data/alpaca_data.json \
    --save_path data/alpaca_data.jsonl 
```

å¯¹æ–‡æœ¬è¿›è¡Œtokenize,åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼Œæ–‡æœ¬é•¿åº¦å¯æ ¹æ®è¿è¡Œèµ„æºè‡ªè¡Œè®¾ç½®

```shell
python tokenize_dataset_rows.py \
    --jsonl_path data/alpaca_data.jsonl \
    --save_path data/alpaca \
    --max_seq_length 320
```

### 2. æ¨¡å‹è®­ç»ƒ

```shell
python train_lora.py \
    --dataset_path data/alpaca \
    --lora_rank 8 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 1 \
    --max_steps 52000 \
    --save_steps 1000 \
    --save_total_limit 2 \
    --learning_rate 2e-5 \
    --fp16 \
    --remove_unused_columns false \
    --logging_steps 50 \
    --output_dir output
```

## å¾®è°ƒ2:BELLEä¸­æ–‡æŒ‡ä»¤æ•°æ®

åŒ…å«543314æ¡ç”±BELLEé¡¹ç›®ç”Ÿæˆçš„ä¸­æ–‡æŒ‡ä»¤æ•°æ®,æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼š

|input| target |
|----|----|
|ç”¨ä¸€å¥è¯æè¿°åœ°çƒä¸ºä»€ä¹ˆæ˜¯ç‹¬ä¸€æ— äºŒçš„ã€‚\n  |   åœ°çƒä¸Šæœ‰é€‚å®œç”Ÿå‘½å­˜åœ¨çš„æ¡ä»¶å’Œå¤šæ ·åŒ–çš„ç”Ÿå‘½å½¢å¼|

> æ•°æ®é›†åœ°å€ï¼šhttps://huggingface.co/datasets/BelleGroup/generated_train_0.5M_CN

### 1.æ•°æ®é¢„å¤„ç†

è½¬åŒ–bellæ•°æ®é›†ä¸ºjsonl

```shell

python cover_alpaca2jsonl.py \
    --dataset_name BelleGroup/generated_train_0.5M_CN \
    --save_path data/belle_data.jsonl 
```

æ–‡æœ¬é•¿åº¦ç»Ÿè®¡

```text
count    543314.000000
mean         83.536944
std          95.665178
min           4.000000
25%          33.000000
50%          51.000000
75%          88.000000
90%         194.000000
max        4410.000000
Name: input_len, dtype: float64

count    543314.000000
mean        121.079030
std         165.472722
min           1.000000
25%          27.000000
50%          67.000000
75%         151.000000
90%         296.000000
max        9463.000000
Name: target_len, dtype: float64
```

åˆ†è¯å¤„ç†

```shell
python tokenize_dataset_rows.py \
    --jsonl_path data/belle_data.jsonl \
    --save_path data/belle \
    --max_seq_length 320
```

è½¬æ¢åçš„æ•°æ®ï¼š

```text
                                           input_ids  seq_len                                                                                                                   
0  [20005, 92863, 20012, 20005, 83864, 87784, 871...       20
1  [20005, 92863, 20012, 20005, 91432, 86523, 885...       80
2  [20005, 92863, 20012, 104069, 85056, 86334, 89...       61
3  [20005, 92863, 20012, 91492, 89122, 83866, 852...       24
4  [20005, 92863, 20012, 20005, 83834, 99899, 927...       24
```

### 2. æ¨¡å‹è®­ç»ƒ

- åŸºäºåŸå§‹chatglm-6bè®­ç»ƒ

```shell
python train_lora.py \
    --dataset_path data/belle \
    --lora_rank 8 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 1 \
    --max_steps 52000 \
    --save_steps 1000 \
    --save_total_limit 2 \
    --learning_rate 2e-5 \
    --fp16 \
    --remove_unused_columns false \
    --logging_steps 50 \
    --output_dir output
```

- åŸºäºalpacaçš„loraç»§ç»­å¾®è°ƒ

```shell
python train_lora.py \
    --dataset_path data/belle \
    --lora_rank 8 \
    --per_device_train_batch_size 8 \
    --gradient_accumulation_steps 1 \
    --max_steps 52000 \
    --save_steps 10000 \
    --save_total_limit 2 \
    --learning_rate 2e-5 \
    --fp16 \
    --remove_unused_columns false \
    --logging_steps 50 \
    --output_dir output/belle \
    --is_resume True \
    --resume_path output/alpaca/chatglm-lora.pt
```

## å¾®è°ƒ3:åŸºäºDeepSpeedè¿›è¡ŒLoraå¾®è°ƒ
æ”¯æŒå¤šå¡+zeroæ–¹æ¡ˆï¼Œè®­ç»ƒé€Ÿåº¦å¯æé«˜8å€å·¦å³

```shell
accelerate launch --config_file config/default_config.yaml train_new.py
```

## å®éªŒç¯å¢ƒ

- å®‰è£…æ‰€éœ€è¦çš„åŒ…ï¼špip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
- æ˜¾å¡ï¼š2xA100 80G

## å®éªŒç»“æœ
- è®­ç»ƒå¥½çš„loraæƒé‡
```text
â””â”€output
    â”œâ”€alpaca:åŸºäº52kå¾®è°ƒçš„loraæƒé‡
    â”œâ”€belleï¼š:åŸºäº52kå¾®è°ƒçš„loraæƒé‡+belleå¾®è°ƒçš„æƒé‡52000steps
    â””â”€belle_rawï¼šbelleå¾®è°ƒçš„æƒé‡104000steps

```


```

é“¾æ¥ï¼šhttps://pan.baidu.com/s/1c-zRSEUn4151YLoowPN4YA?pwd=hxbr
æå–ç ï¼šhxbr
--æ¥è‡ªç™¾åº¦ç½‘ç›˜è¶…çº§ä¼šå‘˜V3çš„åˆ†äº«
```


- alpacaæ•°æ®å¾®è°ƒæ•ˆæœ

![](https://raw.githubusercontent.com/yanqiangmiffy/InstructGLM/master/examples/alpaca.png)

- belleæ•°æ®å¾®è°ƒæ•ˆæœ

![](https://raw.githubusercontent.com/yanqiangmiffy/InstructGLM/master/examples/belle.png)

## Todo
* [ ] deepspeedæ”¯æŒ
* [ ] æ¨¡å‹è¯„ä¼°

## Reference

> éå¸¸æ„Ÿè°¢ä»¥ä¸‹ä½œè€…çš„æ— ç§å¼€æº

- https://github.com/mymusise/ChatGLM-Tuning
- https://huggingface.co/BelleGroup/BELLE-7B-2M
- https://github.com/LianjiaTech/BELLE
- https://huggingface.co/datasets/BelleGroup/generated_train_0.5M_CN
- https://huggingface.co/datasets/JosephusCheung/GuanacoDataset
- https://guanaco-model.github.io/
- https://github.com/carbonz0/alpaca-chinese-dataset
- https://github.com/THUDM/ChatGLM-6B
- https://huggingface.co/THUDM/chatglm-6b
- https://github.com/lich99/ChatGLM-finetune-LoRA

## Bugs

- gccç‰ˆæœ¬å‡çº§
```
yum install centos-release-scl -y
yum install devtoolset-9 -y

#ä¸´æ—¶è¦†ç›–ç³»ç»ŸåŸæœ‰çš„gccå¼•ç”¨
scl enable devtoolset-9 bash

# æŸ¥çœ‹gccç‰ˆæœ¬
gcc -v
```
